# Benchmarking Blockchain application

## Introduction
The goal of benchmarking the developed Blockchain application is to understand its performance and scalability properties.
In particular, we want to estimate the amount of users/work that the system can support simultaneously.
In this report are presented the plots and the measurements obtained with the Tsung benchmarking tests, the queueing network and its analysis performed with JMT (*Java Modelling Tools*).
Each result reported below corresponds to the average of several benchmarking tests (about 10 per result) each one with a duration of 30 minutes.
Each test was performed with two computers: one in which was running the Blockchain application and one in which was running the Tsung benchmarking tool to test the performance of the application.

## Benchmarking tests
In this section are explained and illustrated the Tsung configurations for the benchmarking tests, the results obtained, some comparisons with theoretical results, several analysis and computations to determine the most important parameters of the queueing network and the study with JMT in order to make decisions to optimise or increase the system performance.

In order to measure the performance of all the implemented features in the Blockchain application, we developed a single Tsung session with a single transaction in which, with equal probability, one of the four main functions of the application is selected and the respective requests are sent with parameters randomly selected from a pool.

 1. Imagining the system as a monolithic queue, we set up a closed system benchmark in low load condition, with a maximum number of users set to 1.
The single user session simply repeats the main transaction N times with no thinking time.
The expected service time obtained is **9.34 sec** with a variance of **29.61** (the variance of all the transactions generated by Tsung).

 2. Now the benchmarking tests were performed as an open system, always imagined as a monolithic queue, with different workload intensities: 0.3L, 0.5L, 0.8L and 0.85L, where L is the arrival rate determined from the expected service time estimated at point 1 (arrival rate = **0.107 j/sec**). Below the results obtained.

 | Workload intensity | Expected response time |
|:------------------:|:----------------------:|
|         0.3        |          14.73         |
|         0.5        |          21.89         |
|         0.8        |          48.83         |
|        0.85        |          64.39         |

Comparing the measurements obtained with the theoretical lines predicted by the M/G/1 and M/G/1/PS queueing systems we obtained the follow results.

<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/MG1%20-%20MG1PS%20comparison.png?raw=true" width="500">

The obtained results are not surprising:

-   our application works better than a M/G/1 queue because some types of scheduling are introduced by the OS and the Flask framework, thus the small requests are served in less time with respect to the FCFS (they do not have to wait the completion of all the previous jobs)

-   since our application is characterized by high variance of the expected service time, there must be the case that it is worse than the M/G/1/PS queue. In fact, the expected response time of the latter is not affected by the variance

 3. In the image below there is our proposal for the queueing network model of the application. There are three stations: **Q<sub>1</sub>** which is an infinite server (modelling the thinking time of the requests), **Q<sub>2</sub>** which represents the CPU (Front-end + Blockchain Server) and **Q<sub>3</sub>** which is the physical disk.

<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/queueing%20network.png?raw=true" width="500">

The proposed routing decision is based on the server implementation and by how the single requests are handled: there are types of request that deterministically do not access the disk, requests that make zero to M disk requests, and lastly requests that make deterministically access the disk M times (M is related to the number of blocks stored in the blockchain).

### Traffic Equations
The queueing network was solved by approximating the relative visit ratio of the disk.
The probability **p=0.62** was obtained by dividing the total number of executed requests by the number of visits that the application did to the disk.
From the traffic equations, it is possible to obtain the relative visit ratio of all the queues: **e<sub>1</sub>=1**, **e<sub>2</sub>=2.65**, **e<sub>3</sub>=1.65**.
From the Tsung report it is possible to compute the throughput of the station Q<sub>1</sub> by dividing the total number of requests performed by the total time of the test (*1800 sec*), obtaining **X<sub>1</sub>=0.134 pages/sec** (from now, *“pages”* are written as *“job”* for simplicity).
With the *Forced Flow Law* it is possible to obtain **X<sub>2</sub>=0.355 j/sec** and **X<sub>3</sub>=0.221 j/sec**.

### Computing service rates
Invoking and recording the output of the `top` command for all the duration of the tests, it was possible to obtain the utilisation of the CPU (U<sub>2</sub>=8.9%) and then compute the service rate of the CPU, obtaining **$\mu$<sub>2</sub>=2.44 j/sec**.
By adding a logger on the function used to load blocks from the disk we recorded the service time of the disk in low load.
The expected service time of the disk (**S<sub>3</sub>=1.79 sec**) was computed as the mean of the obtained values.
The respective service rate of the disk is **$\mu$<sub>3</sub>=0.56 j/sec**.

Now it is possible to compute the service demand for Q<sub>2</sub> and Q<sub>3</sub>, respectively **D<sub>2</sub>=1.08** and **D<sub>3</sub>=2.95**.

 4. From the previous point, it is possible to conclude that the bottleneck of the system is the station Q<sub>3</sub> (the disk), so it is possible to compute the maximum throughput of the system which is **X<sub>1</sub><sup>MAX</sup>=0.339 j/sec**.
By choosing a reasonable thinking time (*Z=10 sec*), the bounds of throughput and expected response time are respectively:
> X $\le$ min{ $\frac{N}{\overline{D}+Z}$, $\frac{1}{D_{3}}$ }
> R $\ge$ max{ $\overline{D}$, $N*D_{3}-Z$ }

where $\overline{D}$ is the summation of all the service demands (reference station excluded).

<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%204/throughput.png?raw=true" width="500">
<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%204/response.png?raw=true" width="500">


With the operational analysis it is possible to determine the maximum level of multiprogramming which corresponds to **N=4.75**.

We also performed some additional open system benchmark in order to check the validity of the maximum throughput derived from the bottleneck. As the arrival rate approached X<sub>1</sub><sup>MAX</sup> the reported disk utilization consistently peaked at 100% causing system freezes and instability while at about 0.5*X<sub>1</sub><sup>MAX</sup> the disk utilization only showed peaks of 60-70%.

 5. With JMT and the *Mean Value Analysis* (MVA) it is possible to study the queueing network with one class and CPU as load independent station from the graphs below (*Q<sub>1</sub> in red*, *Q<sub>2</sub> in blue*, *Q<sub>3</sub> in green*).

<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205/throughput.png?raw=true" width="300"> <img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205/number%20of%20customers.png?raw=true" width="300">

<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205/residence%20time.png?raw=true" width="300"> <img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205/utilisation.png?raw=true" width="300">

## Optimisations
Now are presented some possible actions to increase the maximum level of multiprogramming of the system.

- **Action 1**: add a new identical disk (Station 4) which works in parallel with the existing one (Q<sub>4</sub> in yellow).

<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205_opt2/throughput.png?raw=true" width="300"> <img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205_opt2/number%20of%20customers.png?raw=true" width="300">

<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205_opt2/residence%20times.png?raw=true" width="300"> <img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205_opt2/utilisation.png?raw=true" width="300">

It is possible to see that both the disks (Q<sub>3</sub> and Q<sub>4</sub>) reach an utilisation equal to 80% when there are about *12 users*, a good improvement with respect to the original result.

- **Action 2**: add a third identical disk (Station 5) which works in parallel with the two existing ones (Q<sub>5</sub> in black).

<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205_opt3/throughput.png?raw=true" width="300"> <img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205_opt3/number%20of%20customers.png?raw=true" width="300">

<img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205_opt3/residence%20times.png?raw=true" width="300"> <img src="https://github.com/narder-davide/BlockchainApp/blob/master/Relation/images/Point%205_opt3/utilisation.png?raw=true" width="300">

It is possible to see that with three disks, the utilisation grows to 80% when there are about *20 users* in the system: a very good improvement.
However, it is possible to see that the utilisation of the CPU grows faster than the utilisation of the disks, *making the CPU the new bottleneck*.

- **Action 3**: Flask multithreaded

Our web server was configured to only use a single thread, causing only one request to be serviced at a time.
By allowing Flask to spawn multiple threads, the Web server queue can be substituted by a *M/G/K* queue with K the number of cpu cores.

|  N | System Response Time (Thinktime 10 sec) | CPU Utilization | Disk Utilization (Mean) |
|:--:|:---------------------------------------:|:---------------:|:-----------------------:|
| 15 |                   9.60                  |       0.74      |           0.68          |
| 20 |                  13.30                  |       0.83      |           0.75          |
| 25 |                  17.54                  |       0.87      |           0.78          |

## Conclusion
The study of the performance of a system allows to compute the maximum workload that the system can support or tolerate simultaneously. Moreover, it allows to derive the bottleneck of the developed system and take actions to design a better queueing network that could support an higher workload in order to limit the number of crashes or unwanted slowdowns and saturations in the queues, always taking care that solving a bottleneck can lead to a generation of a new bottleneck in a different queue.
